# 基于强化学习的智能定价
   本项目是**中远海运定价系统**的**强化学习定价模块**的一个仿真测试程序。我们使用强化学习的方法来训练模型，并进行一个仿真实验对模型进行测试。
## 基本原理的介绍
   我们只考虑时间、舱位利用率对定价产生影响。很自然地，当距离开船日期比较久时，我们可以适当地提高定价来增大收益；当距离开船日期很近时，我们应当适当地降低定价从而在开船日期截止前吸引更多的用户以获得较大的收益。当舱位利用率比较低时，我们应当适当地降低定价来吸引客户，从而提高舱位利用率获得较高的收益；当舱位利用率较高时，我们应当适当地提高定价来获得更大的收益。这个思想是非常简单并且；朴素的，然而如何具体地来定价确是一件较为困难的事情。当距离开船日期比较久、舱位利用率比较高时，很自然地我们应该定一个比较高的定价，这样才能获得比较大的收益；当距离开船截至日期比较近、舱位利用率较低时，很显然我们应该定一个较低的价格，因为这样才能尽快的去库存以获得尽可能大的收益。然而这个较高的定价和较低的定价具体是多少我们就比较难确定了。凭感觉来定价很有可能会产生较大的误差。另外，当距离开船日期比较久、舱位利用率比较低时应该如何定价也是一个难以确定的问题。从时间来看，我们应该定一个比较高的价格，从舱位利用率来看，我们应该定一个比较低的价格。这样就自相矛盾了，如何在时间和舱位利用率之间做一个取舍是一件头疼的事。而上面的问题正好可以利用强化学习来解决。<br>
   
   **强化学习的最终目标是学习出一个策略，智能体采取这一策略可以获得最大期望累积回报。很显然，在我们的项目中，这个智能体就是定价系统，而它的定价策略就是需要学习出的目标，累积回报是卖出去的舱位的总收益**。所谓策略是指智能体在所有给定状态下采取不同动作的概率。在本项目中，状态空间用三元组***（天数day，舱位利用率ratio，定价price）***表示，动作空间共有三个动作 ***(plus,decrease,unchanged)*** 分别表示在现有定价的基础上增加百分之十、减少百分之十、不变。当然我们假设价格是在一个区间内变化的，不会超过这个区间的边界。因此，从直观上来理解，我们要学习的目标就是在某一时刻、舱位利用率确定、当前价格已知时采取加价、减价、保持价格不变这三种动作各自的概率。<br>
   
   **我们假设总天数为20天，也就是说对于一条航线我们从开船日期的前二十天开始进行定价预测（当然这个参数可以随便定，也能定成小时，这次是一个实验性的程序因此为了图方便我就定成了20天，未来根据项目需要可以对它进行修改），总舱位为200个舱位，基础价格为100，价格区间为[81,130]，每天想要运输货物的客户数服从均值为15的泊松分布，每个用户能够接受的最高价格服从均值为100，方差为15的正态分布（这是一个仿真实验，客户模型是我们假设的，具体实现的时候需要基于中远的数据预测出正确的客户模型，这是项目当前最大的难点）**。<br>
   
 ## 具体实现介绍
   我们的程序一共包括两个文件，分别是training.py和comparing.py。training.py中包含如何基于强化学习的方法训练模型的函数，comparing.py则是对模型的效果进行一个测试。<br>
   
   下面介绍程序的具体细节。
### training.py介绍
training.py包中一个包括6个函数，分别是：load_func(),epsilon_greedy(qfunc,day,price,inventory,epsilon),update_price(price,action),Q_learning(qfunc,episodes=0),output_qfunc(qfunc),output_policy(qfunc)。下面我对这6个函数分别介绍。<br>
    
 load_qfunc()从‘my_pickle.pkl'文件导入qfunc，我们将训练好的模型的参数qfunc保存在文件’my_pickle.pkl'中，训练时首先从这个文件将模型参数导出来，如果没有‘my_pickle.pkl'则创建‘my_pickle.pkl'，并随机初始化qfunc，然后返回qfunc。<br>
 
epsilon_greedy(qfunc,day,price,inventory,epsilon)是Q_learning算法中经常要用到的一个函数，它输入qfunc和当前的状态（day是时间，price是当前价格，inventory是库存数，用以计算舱位利用率），epsilon是一个自定义的学习参数，一般设为0.15。它返回该状态执行的动作。<br>

Q_learning(qfunc,episodes=0）是训练模型的函数，输入模型的Q值qfunc以及训练的次数episodes，函数返回训练后模型的qfunc。<br>

output_qfunc(qfunc)函数用于输出模型的qfunc，这是用来查看模型训练效果的函数。<br>

output_policy(qfunc)函数用于输出模型的策略，这是用来查看模型最终训练出来的策勒的函数。<br>
    

### comparing.py介绍
   在comparing.py模块中，我们将模型学习出来的定价策略与随机定价策略（在81-130之间随即定价）和确定定价策略（价格一直定为100）进行对比，看看我们的模型效果怎么样。我们进行了仿真实验，模拟了20天每天的客户数以及每个客户所能接受的最高价格，计算采取不同定价策略所能获得的累积回报。结果表明，我们的模型训练出来的策略要优于随即定价策略和确定定价策略。
